## 深層学習の概要

深層学習という単語は近頃様々な場所で聞くようになりました。深層学習は既存の多くの課題を高い水準で
解くことができる可能性を持っており、現在最も盛んな研究分野の一つと言えます。
深層学習について簡単に説明します。ニューラルネットワークと呼ばれる層状の構造を多数組み合わせた
ネットワークを構築し、そのようなネットワークに対して入力データを渡すことで出力を得ます。その出力
と望ましい出力を比較してその差をネットワークに反映させることで、少しずつネットワークの性能を向上
させます。このプロセスがニューラルネットワークを用いた機械学習の基本となっており、層の数が十分に 大きいものを深層学習と呼びます。
ニューラルネットワークを用いた機械学習の研究者の方々によって、層の数を増やしてネットワークの性能
を向上させる試みは成功しつつあります。しかしその一方で計算回数が増えつつあります。 例えば深層学習を用いた画像認識の分野で大きな影響を持つ
ResNet-50 [^1]というネットワークの例だと、サイズが 224×224 の画像に対して分類を行うのに 38
億回の計算（浮動小数点演算）が必要に なります。更に ResNet-50 を学習させるのに一般的に用いられるデータセットには
ImageNet [^2] があるのですが、学習用に用いる画像の枚数は 120 万枚にものぼります。 つまり上記の 38
億回の計算が大量に行われるため、学習には極めて時間がかかってしまいます。

## 深層学習フレームワーク

話は少し変わりますが、深層学習技術で研究・実験する際には作成したいネットワークの構成を実際にプログラム
として実装する必要があります。この実装を簡単にするために一般的には深層学習フレームワークと呼ばれる ツールを用います。現在よく使われるフレームワークには
TensorFlow, PyTorch, Caffe, Chainer
などがあります。 上記のような極めて計算回数の多いネットワークも研究・実験の際にはこれらのツールを用いることが多いのですが、
そうなるとツールを用いたプログラムには高速に実行してほしくなります。 このような高速化には GPU
と呼ばれる画像処理用プロセッサを用いた計算技術や、私達が普段使っている CPU を
効率的に用いる技術などが研究されていますが、やはりプログラムに対して「簡単な実装」と「高速な実行」を 両立させることは容易なことではありません。

## 研究内容

以上の流れを踏まえて、私は深層学習フレームワークそのものの性能解析を行っています。ソフトウェアの高速化
においてどこが遅いのか、を調べることは最も大切なことの一つであるためです。 現在は主に CPU
における性能解析を行っており、上記のツール群を実装するプログラミング言語に由来する問題から、 フレームワークの設計上発生する問題などについて調べています。

[^1]: K. He, X. Zheng, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016.
[^2]: Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L. ImageNet: A Large-Scale Hierarchical Image Database. 2009.

